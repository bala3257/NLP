{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "import codecs\n",
    "import re\n",
    "import sys\n",
    "from nltk.tokenize import word_tokenize               # <=== tokenizer \n",
    "from nltk.stem.porter import PorterStemmer            # <=== stemmer \n",
    "from nltk.corpus import stopwords as nltk_stopwords   # <=== stopwords\n",
    "STOPWORDS = set(nltk_stopwords.words('english'))\n",
    "# print(re.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= Valkyria Chronicles III = Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside\n",
      "= Tower Building of the Little Rock Arsenal = The Tower Building of the Little Rock Arsenal , also known as U.S. Arsenal Building , is a building located in MacArthur Park in downtown Little Roc\n"
     ]
    }
   ],
   "source": [
    "text = codecs.open('data/SearchEngine/wiki-600', encoding='utf-8').read()\n",
    "starts = [match.span()[0] for match in re.finditer('\\n = [^=]', text) ]\n",
    "\n",
    "articles = list()\n",
    "for ii , start in enumerate(starts):\n",
    "  end = starts[ii + 1] if ii+1 < len(starts) else len(text)\n",
    "  articles.append(text[start:end])\n",
    "    \n",
    "\n",
    "snippets = [' '.join(article[:200].split()) for article in articles]\n",
    "\n",
    "\n",
    "for snippet in snippets[:2]:\n",
    "    print(snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculating term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/balaji/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/balaji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "STOPWORDS = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize the articles, calculate term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, term_frequency for \"einstein\"\n",
      "{84: 5, 294: 1, 300: 1}\n",
      "========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from nltk.tokenize import word_tokenize               # <=== tokenizer \n",
    "from nltk.stem.porter import PorterStemmer            # <=== stemmer \n",
    "from nltk.corpus import stopwords as nltk_stopwords   # <=== stopwords\n",
    "\n",
    "TABLE = dict( [(ord(cc), ord(' ')) for cc in string.punctuation] )\n",
    "STOPWORDS = set(nltk_stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def get_tokens(article):\n",
    "    article = str(article)\n",
    "    article = article.translate(TABLE) # remove punctuation\n",
    "    tokens = word_tokenize(article) # tokenize\n",
    "    tokens = [token.lower() for token in tokens] # normalization --> to make it case insensitive search\n",
    "    tokens = [token for token in tokens if not token in STOPWORDS] # remove stop words\n",
    "    tokens = [stemmer.stem(token) for token in tokens]  # stemming\n",
    "    return tokens\n",
    "\n",
    "\n",
    "term_frequency = defaultdict(dict)\n",
    "\n",
    "def index(id, article):\n",
    "    tokens = get_tokens(article)\n",
    "    tokens = Counter(tokens)\n",
    "    for token, frequency in tokens.items():\n",
    "        term_frequency[token][id] = frequency\n",
    "\n",
    "for ii, article in enumerate(articles):\n",
    "    if ii and ii%10 == 0: print(ii, end=', ')\n",
    "    sys.stdout.flush()\n",
    "    index(ii, article)\n",
    "\n",
    "print('term_frequency for \"einstein\"')\n",
    "print(term_frequency['einstein'])\n",
    "print('='*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving .. \n",
      "Done\n",
      "Loading .. \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def picklesave(obj, filename):\n",
    "    print('Saving .. ')\n",
    "    ff = open(filename, 'wb')\n",
    "    pickle.dump(obj, ff)\n",
    "    ff.close()\n",
    "    print('Done')\n",
    "    return True\n",
    "\n",
    "def pickleload(filename):\n",
    "    print('Loading .. ')\n",
    "    ff = open(filename, 'rb')\n",
    "    obj = pickle.load(ff)\n",
    "    ff.close()\n",
    "    print('Done')\n",
    "    return obj\n",
    "\n",
    "picklesave([snippets, term_frequency], 'data-26000.pdata')\n",
    "snippets, term_frequency = pickleload('data-26000.pdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You search for: \"Smart phone\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "= In Bloom = For the 2013 film of the same name , see In Bloom ( 2013 film ) \" In Bloom \" is a song by American rock band Nirvana . Written by frontman Kurt Cobain , the song addresses people\n",
      "= Bart vs. Australia = \" Bart vs. Australia \" is the sixteenth episode of the sixth season of The Simpsons . It originally aired on the Fox network in the United States on February 19 , 1995 . I\n",
      "= Crash Boom Bang ! = For the Roxette album with a similar name , see Crash ! Boom ! Bang ! Crash Boom Bang ! ( known in Japan as Crash Bandicoot Festival ) ( クラッシュ ・ バンディクー フェスティバル , Kurasshu\n",
      "====================================================================================================\n",
      "You search for: \"einstein\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "= Edward Creutz = Edward Creutz ( January 23 , 1913 – June 27 , 2009 ) was an American physicist who worked on the Manhattan Project at the Metallurgical Laboratory and the Los Alamos Laboratory\n",
      "= Bob Dylan = Bob Dylan ( / ˈdɪlən / ; born Robert Allen Zimmerman , May 24 , 1941 ) is an American singer @-@ songwriter , artist and writer . He has been influential in popular music and cultu\n",
      "= Transit of Venus = A transit of Venus across the Sun takes place when the planet Venus passes directly between the Sun and a superior planet , becoming visible against ( and hence obscuring a\n",
      "====================================================================================================\n",
      "You search for: \"physics\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "= Edward Creutz = Edward Creutz ( January 23 , 1913 – June 27 , 2009 ) was an American physicist who worked on the Manhattan Project at the Metallurgical Laboratory and the Los Alamos Laboratory\n",
      "= Jane 's Attack Squadron = Jane 's Attack Squadron is a 2002 combat flight simulator developed by Looking Glass Studios and Mad Doc Software and published by Xicat Interactive . Based on World\n",
      "= Frederick Reines = Frederick Reines ( RYE @-@ ness ) ; ( March 16 , 1918 – August 26 , 1998 ) was an American physicist . He was awarded the 1995 Nobel Prize in Physics for his co @-@ detectio\n",
      "====================================================================================================\n",
      "You search for: \"india\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "= Independence Day ( India ) = Independence Day , observed annually on 15 August is a national holiday in India commemorating the nation 's independence from the British Empire on 15 August 1947\n",
      "= Mortimer Wheeler = Sir Robert Eric Mortimer Wheeler CH , CIE , MC , TD , FSA , FRS , FBA ( 10 September 1890 – 22 July 1976 ) was a British archaeologist and officer in the British Army . Over\n",
      "= Varanasi = Varanasi ( Hindustani pronunciation : [ ʋaːˈraːɳəsi ] ) , also known as Benares , Banaras ( Banāras [ bəˈnaːrəs ] ) , or Kashi ( Kāśī [ ˈkaːʃi ] ) , is a North Indian city on the ba\n",
      "====================================================================================================\n",
      "You search for: \"director\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "= Mortimer Wheeler = Sir Robert Eric Mortimer Wheeler CH , CIE , MC , TD , FSA , FRS , FBA ( 10 September 1890 – 22 July 1976 ) was a British archaeologist and officer in the British Army . Over\n",
      "= Laurence Olivier = Laurence Kerr Olivier , Baron Olivier , OM ( / ˈlɒrəns kɜːr ɒˈlɪvieɪ / ; 22 May 1907 – 11 July 1989 ) was an English actor who , along with his contemporaries Ralph Richards\n",
      "= Paul Thomas Anderson = Paul Thomas Anderson ( born June 26 , 1970 ) also known as P.T. Anderson , is an American film director , screenwriter and producer . Interested in film @-@ making at a\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "D = len(snippets)\n",
    "def search(query, nresults=3):\n",
    "    tokens = get_tokens(query)\n",
    "    scores = defaultdict(float)\n",
    "    for token in tokens:\n",
    "        document_frequency = len(term_frequency[token])\n",
    "        for article, score in term_frequency[token].items():\n",
    "            scores[article] += score * math.log(1. * D / document_frequency)\n",
    "    return sorted(scores.keys(), reverse=True, key=scores.get)[:nresults]\n",
    "\n",
    "def display_results(query, results):\n",
    "    print('You search for: \"%s\"' % query)\n",
    "    print('-'*100)\n",
    "    for result in results:\n",
    "        print(snippets[result])\n",
    "    print('='*100)\n",
    "\n",
    "display_results('Smart phone', search('smart phone'))\n",
    "display_results('einstein', search('einstein'))\n",
    "display_results('physics', search('physics'))\n",
    "display_results('india', search('india'))\n",
    "display_results('director', search('director'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
